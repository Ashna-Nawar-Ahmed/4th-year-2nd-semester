{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab 05 - Natural Language Processing (NLP) - Basic Terminologies.ipynb","provenance":[{"file_id":"1x8MB0ZNgCKI1co3YHSG_U7TUabNE6bBO","timestamp":1614210843608},{"file_id":"13Y75B4h5-4U5W3eHv6SsqjOo4vuN9ZM6","timestamp":1593381550713}],"collapsed_sections":[],"authorship_tag":"ABX9TyM4UzfgVI1plIz0PWt47Z+a"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5CjQDebxMtjG"},"source":["## Natural Language Processing (NLP) - A hands-on introduction"]},{"cell_type":"markdown","metadata":{"id":"UuPcELhlLCJB"},"source":["### Popular Libraries\n","\n","- [NLTK](https://www.nltk.org/)\n","- [spaCy ](https://spacy.io/)\n","\n","**NLTK & spaCy** is a free open-source library for Natural Language Processing (NLP) in Python to support teaching, research, and development. Which are:- \n","  - Free and Open source\n","  - Easy to use\n","  - Modular\n","  - Well documented\n","  - Simple and extensible\n","\n","--------------------------\n","\n","* In this notebook, I will provide basic NLP tasks that we need in order to **process raw text to find useful informations**. \n","* For each tasks, we will be using **NLTK as well as spaCy**. Good news is that both are installed in Google Colab by default. "]},{"cell_type":"markdown","metadata":{"id":"yvxtJdF1RYUT"},"source":["### Some definitions\n","\n","- **Corpus** - Corpora is the plural of Corpus. **\"Corpus\"** mainly appears in NLP area or application domain related to **texts/documents**, because of its meaning **\"a collection of written texts\"**\n","    - **Example:** A collection of news documents.\n","\n","- **Dataset** - dataset appears in every application domain (in can be **image/video/text/numerical/mixed**) --- a collection of any kind of data is a dataset.\n","\n","- **Lexicon** - vocabulary or list of Words and their meanings.\n","    - **Example:** English dictionary.\n","\n","- **Token** - Each \"entity\" that is a part of whatever was split up based on\n","rules.\n","    - For examples, each word is a token when a sentence is \"tokenized\" into\n","words. Each sentence can also be a token, if you tokenized the sentences out\n","of a paragraph."]},{"cell_type":"markdown","metadata":{"id":"wZFH748nqbZ8"},"source":["## Tokenization\n","\n","Tokenization is the process of breaking a stream of text up into **sentences, words, phrases, symbols, or other meaningful elements called tokens**.\n"]},{"cell_type":"code","metadata":{"id":"2E3OsfqlqqBl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614672134873,"user_tz":-360,"elapsed":1660,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"e32e1516-bb91-48d0-f6a8-0ac897927dc0"},"source":["import nltk\n","nltk.download('punkt')\n","\n","# For tokenizing words and sentences\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","s = \"Good muffins cost $3.88\\nin New York. Please buy me two of them.\\n\\nThanks.\"\n","\n","print (sent_tokenize(s))\n","print (word_tokenize(s))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","['Good muffins cost $3.88\\nin New York.', 'Please buy me two of them.', 'Thanks.']\n","['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9B49lU_b1XGN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614672371935,"user_tz":-360,"elapsed":1571,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"9bf6a9f9-6904-4a67-d2a4-8346ac05ab19"},"source":["import spacy\n","\n","# Small spaCy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","doc = nlp(\"Good muffins cost $3.88\\nin New York. Please buy me two of them.\\n\\nThanks.\")\n","\n","print(\"\\n\\nTokenized Sentences\")\n","\n","for i, sent in enumerate(doc.sents):\n","        print('-->Sentence %d: %s' % (i, sent.text))\n","\n","print(\"\\n\\nTokenized Words\")\n","\n","tokens = [token.text for token in doc]\n","print(tokens)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["\n","\n","Tokenized Sentences\n","-->Sentence 0: Good muffins cost $3.88\n","in New York.\n","-->Sentence 1: Please buy me two of them.\n","\n","\n","-->Sentence 2: Thanks.\n","\n","\n","Tokenized Words\n","['Good', 'muffins', 'cost', '$', '3.88', '\\n', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', '\\n\\n', 'Thanks', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Bt-7RAu-8WoL"},"source":["### Downloading Large spaCy model"]},{"cell_type":"code","metadata":{"id":"zACJ-rtS6A-a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614669391153,"user_tz":-360,"elapsed":181398,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"28015b5a-fe72-450f-8be5-92d3fa99def7"},"source":["!python -m spacy download en_core_web_lg\n"," \n","import en_core_web_lg\n"," \n","nlp = en_core_web_lg.load()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting en_core_web_lg==2.2.5\n","\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n","\u001b[K     |████████████████████████████████| 827.9MB 1.2MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (53.0.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.0)\n","Building wheels for collected packages: en-core-web-lg\n","  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp37-none-any.whl size=829180944 sha256=8a5ac941d22f1b672be9a7dc3d254afcc459b9df0521dcee735b8386f09ca71d\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-71k_i5rh/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n","Successfully built en-core-web-lg\n","Installing collected packages: en-core-web-lg\n","Successfully installed en-core-web-lg-2.2.5\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_lg')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PIMjiA98_6Km"},"source":["## Filtering stopwords\n","\n"," - **Stopwords** are common words that **generally** do not contribute to the\n","meaning of a sentence.\n"," - Most search engines will filter stopwords out of search queries and\n","documents in order to **save space and time** in their index.\n","- Removing stopwords is not a hard and fast rule in NLP. It depends upon the task that we are working on. \n","- For tasks like text classification, where the text is to be classified into different categories, stopwords are removed or excluded from the given text so that **more focus can be given to those words which define the meaning of the text.**\n"," - All [Stopwords](https://github.com/stopwords-iso/stopwords-iso) collection including **Bengali**."]},{"cell_type":"code","metadata":{"id":"d_M90K-HApNi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614673619825,"user_tz":-360,"elapsed":1567,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"c7414d14-1930-420a-e952-6747b1f03909"},"source":["nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","# All english stopwords list\n","english_stops = set(stopwords.words('english'))\n","\n","print (english_stops)\n","\n","print (len(english_stops))\n","\n","words = ['The', 'natural', 'language', 'processing', 'is', 'very', 'interesting']\n","filtered_words = [word for word in words if word.lower() not in english_stops]    # word.lower() is for lowering down the words\n","\n","print(filtered_words)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","{'for', 'o', 'each', 'above', 'won', 'of', 'd', 'on', 'them', \"couldn't\", 'did', 'out', 'he', \"you're\", \"it's\", 'before', 'will', 'their', 'doing', 'can', 'were', 'below', \"weren't\", 'the', 'should', 'very', 'just', 'down', 'so', \"shan't\", 'do', 've', 'both', \"she's\", 's', 'am', 'yourselves', 'shouldn', \"wasn't\", \"won't\", 'i', 'myself', 'a', 'haven', 'mightn', \"you'll\", 'other', 'ain', 'there', 'own', 'an', 'have', 'no', 'such', 'ours', 'up', 'me', 'most', \"isn't\", 'against', 'is', 'had', 'll', \"that'll\", 'theirs', \"hasn't\", 'through', 'than', 'too', 'because', 'nor', 're', 'been', \"shouldn't\", 'herself', \"should've\", 'this', 'which', 'what', \"hadn't\", 'being', 'at', 'itself', 'when', 'why', 'ourselves', 't', 'from', 'not', 'now', 'him', 'between', 'its', 'or', \"don't\", 'her', 'wouldn', 'himself', 'but', 'are', 'whom', 'didn', 'hasn', 'you', 'it', 'having', 'then', 'if', 'in', 'couldn', 'and', 'isn', \"you'd\", 'y', 'we', 'any', 'those', 'hadn', 'yours', 'm', 'with', 'about', 'as', 'was', 'doesn', 'don', 'does', 'until', 'same', 'over', \"didn't\", \"aren't\", 'only', 'aren', 'themselves', \"you've\", 'under', 'that', 'few', \"mustn't\", 'mustn', \"mightn't\", 'yourself', 'further', 'by', 'shan', 'again', \"haven't\", 'weren', 'hers', 'while', 'once', 'after', 'your', 'during', 'our', 'ma', 'these', 'his', 'how', 'more', 'my', \"needn't\", 'some', 'where', 'needn', 'to', 'all', 'off', \"doesn't\", 'be', 'here', 'into', 'wasn', 'has', 'who', \"wouldn't\", 'she', 'they'}\n","179\n","['natural', 'language', 'processing', 'interesting']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c1oeIOcgsWsv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614673730895,"user_tz":-360,"elapsed":1153,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"31718f54-2459-40be-da82-8a3f2948187c"},"source":["spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n","\n","print('Number of stop words: %d' % len(spacy_stopwords))\n","print('First ten stop words: %s' % list(spacy_stopwords)[:10])"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Number of stop words: 326\n","First ten stop words: ['twenty', 'he', 'before', '‘d', 'thence', 'never', \"'s\", 'very', 'down', 'am']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x7F0TnPuqLNs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614673795775,"user_tz":-360,"elapsed":1462,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"eaff89ba-a0c0-4ffa-e595-e2d5133ea2b2"},"source":["doc = nlp(\"Good muffins cost $3.88\\nin New York. Please buy me two of them.\\n\\nThanks.\")\n","\n","tokens = [token.text for token in doc if not token.is_stop]\n","\n","print(tokens)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["['Good', 'muffins', 'cost', '$', '3.88', '\\n', 'New', 'York', '.', 'buy', '.', '\\n\\n', 'Thanks', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QpJM8xKR1ebv"},"source":["## Adding Custom Stopwords\n","\n"]},{"cell_type":"code","metadata":{"id":"tw3p0Sc81sTy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614674027651,"user_tz":-360,"elapsed":1499,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"351f8223-1cbd-4d2b-c050-56c6d7a0b54c"},"source":["\n","english_stops = set(stopwords.words('english'))\n","\n","print (english_stops)\n","\n","english_stops.remove('is')\n","english_stops.add('natural')\n","\n","\n","words = ['The', 'natural', 'language', 'processing', 'is', 'very', 'interesting']\n","filtered_words = [word for word in words if word.lower() not in english_stops]\n","\n","print(filtered_words)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["{'for', 'o', 'each', 'above', 'won', 'of', 'd', 'on', 'them', \"couldn't\", 'did', 'out', 'he', \"you're\", \"it's\", 'before', 'will', 'their', 'doing', 'can', 'were', 'below', \"weren't\", 'the', 'should', 'very', 'just', 'down', 'so', \"shan't\", 'do', 've', 'both', \"she's\", 's', 'am', 'yourselves', 'shouldn', \"wasn't\", \"won't\", 'i', 'myself', 'a', 'haven', 'mightn', \"you'll\", 'other', 'ain', 'there', 'own', 'an', 'have', 'no', 'such', 'ours', 'up', 'me', 'most', \"isn't\", 'against', 'is', 'had', 'll', \"that'll\", 'theirs', \"hasn't\", 'through', 'than', 'too', 'because', 'nor', 're', 'been', \"shouldn't\", 'herself', \"should've\", 'this', 'which', 'what', \"hadn't\", 'being', 'at', 'itself', 'when', 'why', 'ourselves', 't', 'from', 'not', 'now', 'him', 'between', 'its', 'or', \"don't\", 'her', 'wouldn', 'himself', 'but', 'are', 'whom', 'didn', 'hasn', 'you', 'it', 'having', 'then', 'if', 'in', 'couldn', 'and', 'isn', \"you'd\", 'y', 'we', 'any', 'those', 'hadn', 'yours', 'm', 'with', 'about', 'as', 'was', 'doesn', 'don', 'does', 'until', 'same', 'over', \"didn't\", \"aren't\", 'only', 'aren', 'themselves', \"you've\", 'under', 'that', 'few', \"mustn't\", 'mustn', \"mightn't\", 'yourself', 'further', 'by', 'shan', 'again', \"haven't\", 'weren', 'hers', 'while', 'once', 'after', 'your', 'during', 'our', 'ma', 'these', 'his', 'how', 'more', 'my', \"needn't\", 'some', 'where', 'needn', 'to', 'all', 'off', \"doesn't\", 'be', 'here', 'into', 'wasn', 'has', 'who', \"wouldn't\", 'she', 'they'}\n","['language', 'processing', 'is', 'interesting']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HQC1aGc6G148"},"source":["## Edit Distance\n","\n","The edit distance is the number of character changes necessary to\n","transform the given word into the suggested word."]},{"cell_type":"code","metadata":{"id":"xuWv7OGZG5rH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614674166002,"user_tz":-360,"elapsed":1838,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"af9935f3-80a9-4f33-ed27-1d2e36f5d449"},"source":["from nltk.metrics import edit_distance\n","\n","print(edit_distance(\"Birthday\",\"Bday\"))\n","\n","print(edit_distance(\"university\", \"varsity\"))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["4\n","4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nyXhKAnChBtP"},"source":["## Removing Punctuation "]},{"cell_type":"code","metadata":{"id":"DSByg19ehLyk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614674366107,"user_tz":-360,"elapsed":1502,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"23a17464-8a2a-42b7-f61b-b2b23ecfb3bf"},"source":["import string\n","import nltk\n","\n","nltk.download('punkt')\n","\n","puncset = list(string.punctuation)\n","\n","print(puncset)\n","\n","sentence = \"Hun Sen's Cambodian can't People's Party won 64 of the 122 parliamentary seats in party July's elections, short of the two-thirds majority needed to form a government on its own.\"\n","\n","sentence = sentence.lower()\n","print(sentence)\n","sentence = nltk.word_tokenize(sentence)\n","print(sentence)\n","sentence = [i for i in sentence if i not in puncset] # Removing punctuation\n","print(sentence)\n","sentence = [w for w in sentence if w.isalpha()] # Removing numbers and punctuation\n","print(sentence)\n"],"execution_count":31,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n","hun sen's cambodian can't people's party won 64 of the 122 parliamentary seats in party july's elections, short of the two-thirds majority needed to form a government on its own.\n","['hun', 'sen', \"'s\", 'cambodian', 'ca', \"n't\", 'people', \"'s\", 'party', 'won', '64', 'of', 'the', '122', 'parliamentary', 'seats', 'in', 'party', 'july', \"'s\", 'elections', ',', 'short', 'of', 'the', 'two-thirds', 'majority', 'needed', 'to', 'form', 'a', 'government', 'on', 'its', 'own', '.']\n","['hun', 'sen', \"'s\", 'cambodian', 'ca', \"n't\", 'people', \"'s\", 'party', 'won', '64', 'of', 'the', '122', 'parliamentary', 'seats', 'in', 'party', 'july', \"'s\", 'elections', 'short', 'of', 'the', 'two-thirds', 'majority', 'needed', 'to', 'form', 'a', 'government', 'on', 'its', 'own']\n","['hun', 'sen', 'cambodian', 'ca', 'people', 'party', 'won', 'of', 'the', 'parliamentary', 'seats', 'in', 'party', 'july', 'elections', 'short', 'of', 'the', 'majority', 'needed', 'to', 'form', 'a', 'government', 'on', 'its', 'own']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1V_sSlmJHrxy"},"source":["## Normalizing Text\n","\n","The goal of both stemming and lemmatization is to **\"normalize\"** words\n","to their **common base form**, which is useful for many text-processing\n","applications.\n","\n"," - **Stemming** = heuristically removing the affixes of a word, to get its\n","**stem (root)**.\n","    - It is a rule-based process of stripping the suffixes **(“ing”, “ly”, “es”, “s” etc)** from a word\n"," - **Lemmatization** = Lemmatization process involves first determining\n","the part of speech of a word, and applying different normalization\n","rules for each part of speech.\n","\n","Consider:\n"," - I was taking a **ride** in the car.\n"," - I was **riding** in the car.\n","\n","Imagine every word in the English language, every possible tense and affix you\n","can put on a word. **Having individual dictionary entries per version would be highly redundant and inefficient.**\n","\n","- Lisa **ate** the food and washed the dishes.\n","- They were **eating** noodles at a cafe.\n","- Don’t you want to **eat** before we leave?\n","- We have just **eaten** our breakfast.\n","- It also **eats** fruit and vegetables.\n","\n","Unfortunately, that is not the case with machines. **They treat these words differently**. Therefore, we need to normalize them to their root word, which is **“eat”** in our example.\n"]},{"cell_type":"markdown","metadata":{"id":"1NiiSnW4PNds"},"source":["### Stemming\n","\n"," - One of the **most popular** stemming algorithms is the **Porter stemmer**,\n","which has been around **since 1979**.\n"," - Several other stemming algorithms provided by NLTK are **Lancaster\n","Stemmer** and **Snowball Stemmer**."]},{"cell_type":"code","metadata":{"id":"nzHkTXgpPNEx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614675002051,"user_tz":-360,"elapsed":3383,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"d5f63313-df97-4af3-b817-3b35695b29ea"},"source":["from nltk.stem import PorterStemmer\n","\n","stemmer = PorterStemmer()\n","\n","example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n","\n","for w in example_words:\n","  print(stemmer.stem(w))"],"execution_count":33,"outputs":[{"output_type":"stream","text":["python\n","python\n","python\n","python\n","pythonli\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"b3E8K8EvUuFR"},"source":["## Lemmatization\n","\n","Lemmatize takes a part of speech parameter, \"pos.\" **If not supplied,\n","the default is \"noun\".**"]},{"cell_type":"code","metadata":{"id":"5ot6yhd_UxrD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614675071200,"user_tz":-360,"elapsed":2381,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"92d0982d-b312-4601-bb0b-f79e0e6103d4"},"source":["## Lemmatization using NLTK\n","\n","from nltk.stem import WordNetLemmatizer\n","\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","print(lemmatizer.lemmatize('cooking'))\n","print(lemmatizer.lemmatize('cooking', pos='v'))  # noun = n, verb = v, ajdective = a"],"execution_count":34,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","cooking\n","cook\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jnky8ws26MZM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614675118242,"user_tz":-360,"elapsed":1236,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"c34af0ea-95c2-48f0-c855-9e684c4286eb"},"source":["## Lemmatization using spaCy\n","\n","doc = nlp('Jim bought 300 shares of Acme Corp. in 2006.')\n","\n","lemma_words = [] \n","\n","for token in doc:\n","    lemma_words.append(token.lemma_)\n","\n","print(lemma_words)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["['Jim', 'buy', '300', 'share', 'of', 'Acme', 'Corp.', 'in', '2006', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IuZUzIuvlcW3"},"source":["## Comparison between stemming and lemmatizing\n","\n","The major difference between these is, as you saw earlier, **stemming\n","can often create non-existent words**, whereas **lemmas are actual\n","words**, you can just look up in an English dictionary."]},{"cell_type":"code","metadata":{"id":"KPANLhcx-Ul9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614675152805,"user_tz":-360,"elapsed":1398,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"a96c85e4-88b5-4639-d796-0b17a44378aa"},"source":["print(stemmer.stem('believes'))\n","print(lemmatizer.lemmatize('believes'))"],"execution_count":36,"outputs":[{"output_type":"stream","text":["believ\n","belief\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LQp_RdbclyuZ"},"source":["## Part-of-speech Tagging\n","\n","The English language is formed of different parts of speech (POS) like nouns, verbs, pronouns, adjectives, etc. POS tagging analyzes the words in a sentences and associates it with a POS tag depending on the way it is used. \n","\n","Full [tag list](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."]},{"cell_type":"markdown","metadata":{"id":"PBd2CLajmxor"},"source":["## Penn Bank Part-of-Speech Tags\n","\n","<div align=\"center\">\n","<img src=\"https://drive.google.com/uc?id=18MqGTRZcK3jYd5Ix8BOaODE-6SGcn_CC\" width=\"700\" height=\"380\">\n","</div>"]},{"cell_type":"code","metadata":{"id":"-ysrGI9Ell3i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614675381132,"user_tz":-360,"elapsed":3418,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"4b56b9dd-2902-47d0-8d60-6e5a2b971d53"},"source":["from nltk.tokenize import word_tokenize\n","from nltk.tag import pos_tag\n","\n","nltk.download('averaged_perceptron_tagger')\n","\n","words = word_tokenize('Jim bought 300 shares of Acme Corp. in 2006.')\n","\n","tagged_words = pos_tag(words)\n","\n","print(tagged_words)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[('Jim', 'NNP'), ('bought', 'VBD'), ('300', 'CD'), ('shares', 'NNS'), ('of', 'IN'), ('Acme', 'NNP'), ('Corp.', 'NNP'), ('in', 'IN'), ('2006', 'CD'), ('.', '.')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"78iSj2027LTU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614675428931,"user_tz":-360,"elapsed":3775,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"c9544f8d-4b19-4f7d-b438-19e7e03577df"},"source":["import spacy\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp('Jim bought 300 shares of Acme Corp. in 2006.')\n","\n","for token in doc:\n","    print(token.text, token.pos_, token.tag_)"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Jim PROPN NNP\n","bought VERB VBD\n","300 NUM CD\n","shares NOUN NNS\n","of ADP IN\n","Acme PROPN NNP\n","Corp. PROPN NNP\n","in ADP IN\n","2006 NUM CD\n",". PUNCT .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W36xzFWEooo0"},"source":["## Named-entity Recognition\n","\n","Named-entity recognition is a subtask of information extraction that\n","seeks to locate and classify elements in text into pre-defined\n","categories such as the names of **persons**, **organizations**, **locations**,\n","**expressions of times**, **quantities**, **monetary values**, **percentages**, etc.\n","\n","**NE Type and Examples:-**\n","\n"," - **ORGANIZATION** - Georgia-Pacific Corp., WHO\n"," - **PERSON** - Eddy Bonte, President Obama\n"," - **LOCATION** - Murray River, Mount Everest\n"," - **DATE**- June, 2008-06-29\n"," - **TIME** - two fifty a m, 1:30 p.m.\n"," - **MONEY** - 175 million Canadian Dollars, GBP 10.40\n"," - **PERCENT** - twenty pct, 18.75 %\n"," - **FACILITY** - Washington Monument, Stonehenge\n"," - **GPE** - South East Asia, Midlothian"]},{"cell_type":"code","metadata":{"id":"KPRLKi8-mCt4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614675511096,"user_tz":-360,"elapsed":2115,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"6c3c0e17-ca86-40ab-b35d-edefec33a544"},"source":["from nltk import pos_tag, ne_chunk\n","from nltk.tokenize import wordpunct_tokenize\n","\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","\n","sent = 'Jim bought 300 shares of Acme Corp. in 2006.'\n","\n","print(ne_chunk(pos_tag(wordpunct_tokenize(sent))))"],"execution_count":40,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n","(S\n","  (PERSON Jim/NNP)\n","  bought/VBD\n","  300/CD\n","  shares/NNS\n","  of/IN\n","  (ORGANIZATION Acme/NNP Corp/NNP)\n","  ./.\n","  in/IN\n","  2006/CD\n","  ./.)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4UB3RkUwpVBU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614675612154,"user_tz":-360,"elapsed":1608,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"b615114c-ce0e-46d7-fbdc-1de08035a960"},"source":["import spacy\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(\"Jim bought 300 shares of Acme Corp. in 2006.\")\n","\n","for ent in doc.ents:\n","    print(ent.text, ent.start_char, ent.end_char, ent.label_)"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Jim 0 3 PERSON\n","300 11 14 CARDINAL\n","Acme Corp. 25 35 ORG\n","2006 39 43 DATE\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6UwTnfmaKoq8"},"source":[""],"execution_count":null,"outputs":[]}]}