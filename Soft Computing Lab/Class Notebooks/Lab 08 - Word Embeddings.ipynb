{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab 08 - Word Embeddings.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM6sKFnKik5LlmEeY0+PaU2"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"E2iZ4xXqrcs6"},"source":["## Word Embeddings\n","\n","- Word embeddings transform a binary/count based or tf*idf vectors into a much smaller dimension vector of real numbers. The one-hot encoded vector or binary vector is also known as a sparse vector, whilst the real valued vector is known as a dense vector. \n","\n","- An word embedding maps discrete, categorical values to a continous space. Major advances in NLP applications have come from these continuous representations of words.\n","\n","- The key concept in these word embeddings is that words that appear in similar contexts appear nearby in the vector space, i.e. the Euclidean distance between these two word vectors is small. \n","\n","- By context here, we mean the surrounding words. For example in the sentences **\"it is the time of stupidity\"** and **\"it is the age of foolishness**\" the words **'time'** and **'age'** and **'stupidity'** and **'foolishness'** appear in the same context and thus should be close together in vector space.\n","\n","- You did learn about word2vec which calculates word vectors from a corpus. In this lab session we use GloVe vectors, GloVe being another algorithm to calculate word vectors. If you want to find out more about GloVe, check the website [here](https://nlp.stanford.edu/projects/glove/). For more information about word embeddings, go [here](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/)."]},{"cell_type":"markdown","metadata":{"id":"WT2xjbPyChNB"},"source":["## Loading the GloVe vectors\n","\n","First, we'll load the GloVe vectors. The name field specifies what the vectors have been trained on, here the 6B means a corpus of 6 billion words. The dim argument specifies the dimensionality of the word vectors. **GloVe vectors are available in 50, 100, 200 and 300 dimensions.** There is also a 42B and 840B glove vectors, **however they are only available at 300 dimensions**.\n","\n","- For more information about GloVe vectors loading using `torchtext` visit the [link](https://torchtext.readthedocs.io/en/latest/vocab.html#glove).\n","\n","- [GLoVe](https://github.com/stanfordnlp/GloVe) comes with different domain differences:-\n","\n","    - **Common Crawl** (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download)\n","    - **Common Crawl** (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)\n","    - **Wikipedia 2014 + Gigaword 5**(6B tokens, 400K vocab, uncased, 300d vectors, 822 MB download)\n","    - **Twitter** (2B tweets, 27B tokens, 1.2M vocab, uncased, 200d vectors, 1.42 GB download)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87-07Kq1OGCl","executionInfo":{"status":"ok","timestamp":1616055627874,"user_tz":-360,"elapsed":1328,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"7d9ecdda-d9c9-4d4b-da48-c82f9fca9a7a"},"source":["import torchtext.vocab\n","\n","glove = torchtext.vocab.GloVe(name = '6B', dim = 100)\n","\n","print(f'There are {len(glove.itos)} words in the vocabulary')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 400000 words in the vocabulary\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JaBn5cZHEJoY"},"source":["As shown above, **there are 400,000 unique words** in the GloVe vocabulary. These are the most common words found in the corpus the vectors were trained on. **In these set of GloVe vectors, every single word is lower-case only.**\n","\n","`glove.vectors is the actual tensor containing the values of the embeddings.`"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NA7bMWI_amGe","executionInfo":{"status":"ok","timestamp":1616055817009,"user_tz":-360,"elapsed":1208,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"f27ae755-296f-47b5-ec52-a250a248097b"},"source":["glove.vectors.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([400000, 100])"]},"metadata":{"tags":[]},"execution_count":100}]},{"cell_type":"markdown","metadata":{"id":"YFCUa5-zH75o"},"source":["We can see what word is associated with each row by checking the **itos (int to string)** list. We can also use the **stoi (string to int)** dictionary, in which we input a word and receive the associated integer/index. If you try get the index of a word that is not in the vocabulary, you receive an error."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FGvd2XvIdDZq","executionInfo":{"status":"ok","timestamp":1616055924184,"user_tz":-360,"elapsed":3553,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"06290aad-2d74-464f-8640-b53f48443975"},"source":["glove.itos[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"]},"metadata":{"tags":[]},"execution_count":101}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QBmvy8bXdSfM","executionInfo":{"status":"ok","timestamp":1616055941615,"user_tz":-360,"elapsed":990,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"2f25b528-9664-45a5-c2a3-e1c7455f924d"},"source":["glove.stoi['the']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":102}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3K5Xd5mPOami","executionInfo":{"status":"ok","timestamp":1616055963841,"user_tz":-360,"elapsed":937,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"2b223bce-1246-4c21-c6a0-8738b091f70a"},"source":["print(glove.vectors[glove.stoi['the']])\n","print(glove.vectors[glove.stoi['the']].shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,\n","        -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,\n","         0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,\n","         0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,\n","         0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,\n","        -0.7179, -0.4153,  0.2033, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,\n","        -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9977, -0.8048, -3.0243,\n","         0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,\n","         1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,\n","        -0.3080, -0.4162,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,\n","         0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,\n","         0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,\n","        -0.5203, -0.1459,  0.8278,  0.2706])\n","torch.Size([100])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g1V3JwLbdZ_E"},"source":["def get_vector(embeddings, word):\n","    assert word in embeddings.stoi, f'*{word}* is not in the vocab!'\n","    return embeddings.vectors[embeddings.stoi[word]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tnp-D7sefc1o","executionInfo":{"status":"ok","timestamp":1616056217274,"user_tz":-360,"elapsed":860,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"c1f243ab-3242-4ab2-b227-f26080e208af"},"source":["print(get_vector(glove, 'dhaka'))\n","print(get_vector(glove, 'dhaka').shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([-0.4313, -0.4264,  0.0080,  0.0843, -0.2632,  0.5497, -0.6609,  1.4093,\n","        -0.3155,  0.9577, -0.2203, -0.5726,  0.3466,  0.2939, -0.0545, -0.6057,\n","         0.1025, -0.2336,  0.3325, -0.5356,  0.9242, -0.4160,  0.9887,  0.0442,\n","         0.1104, -0.2943,  0.3938, -0.2971,  0.0072,  0.6839, -0.5073, -0.0298,\n","         0.0992,  0.3575, -1.0666, -0.3014,  0.0949,  0.1943, -0.7708,  0.1985,\n","        -0.3778,  0.8102,  0.0784, -0.8903,  1.0367,  0.1295, -0.1955,  0.3953,\n","        -0.1357,  0.5108, -0.1412, -0.2397,  0.8553,  0.2163, -0.4538,  0.0355,\n","        -1.1429, -0.3612,  0.8375, -0.0534, -1.3352, -0.1135, -0.7246,  0.1347,\n","        -0.7338,  0.6919, -0.1318, -0.1666,  0.4299,  0.3777,  0.0694, -0.5150,\n","        -0.0721, -0.7482, -0.1416,  0.3478,  0.4706,  0.2370, -0.8630, -0.3583,\n","         0.4200,  0.4044, -0.7176, -0.3392, -0.3349,  0.3887, -0.7387, -0.2911,\n","        -0.1261,  0.5037,  0.9511,  0.1648, -0.5015, -0.1889, -0.4417,  1.2995,\n","         0.9472,  0.0645,  0.4155,  0.8320])\n","torch.Size([100])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NnPjkO86SqW1"},"source":["## Similar Contexts\n","\n","Now to start looking at the context of different words.\n","\n","If we want to find the words similar to a certain input word, we first find the vector of this input word, then we scan through our vocabulary calculating the distance between the vector of each word and our input word vector. We then sort these from closest to furthest away.\n","\n","The function below returns the closest 10 words to an input word vector:"]},{"cell_type":"code","metadata":{"id":"WOQ168R6fgPQ"},"source":["import torch\n","\n","def closest_words(embeddings, vector, n = 10):\n","    \n","    distances = [(word, torch.dist(vector, get_vector(embeddings, word)).item())\n","                 for word in embeddings.itos]\n","    \n","    return sorted(distances, key = lambda w: w[1])[:n]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2fPjFFBVh3A8"},"source":["Let's try it out with 'dhaka'. The closest word is the word 'dhaka' itself (not very interesting), however all of the words are related in some way. \n","\n","Interestingly, we also get 'lahore' and 'karachi', implies that Bangladesh, and Pakistan are frequently talked about together in similar contexts. \n","\n","Moreover, other vectors are geographically situated near each other."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wu6LZp6Yhxml","executionInfo":{"status":"ok","timestamp":1616056388013,"user_tz":-360,"elapsed":4845,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"f2c1eba7-a5db-4e76-8e4b-14267045df93"},"source":["word_vector = get_vector(glove, 'dhaka')\n","\n","closest_words(glove, word_vector)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('dhaka', 0.0),\n"," ('lahore', 3.73711895942688),\n"," ('karachi', 3.852436065673828),\n"," ('calcutta', 3.947979211807251),\n"," ('kathmandu', 3.9504103660583496),\n"," ('bangkok', 3.986726760864258),\n"," ('chittagong', 4.223588466644287),\n"," ('multan', 4.2937541007995605),\n"," ('harare', 4.355865001678467),\n"," ('delhi', 4.358461856842041)]"]},"metadata":{"tags":[]},"execution_count":106}]},{"cell_type":"markdown","metadata":{"id":"LKXpxMeIi7_G"},"source":["Looking at another country, India, we also get nearby countries: Thailand, Malaysia and Sri Lanka (as two separate words). Australia is relatively close to India (geographically), but Thailand and Malaysia are closer. So why is Australia closer to India in vector space? This is most probably due to India and Australia appearing in the context of cricket matches together."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QcED47-mic9n","executionInfo":{"status":"ok","timestamp":1616056562229,"user_tz":-360,"elapsed":4478,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"7db9f5f2-15e0-4647-9bb9-fe1180fb9b27"},"source":["word_vector = get_vector(glove, 'india')\n","\n","closest_words(glove, word_vector)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('india', 0.0),\n"," ('pakistan', 3.6954822540283203),\n"," ('indian', 4.114313125610352),\n"," ('delhi', 4.155975818634033),\n"," ('bangladesh', 4.261017799377441),\n"," ('lanka', 4.435846328735352),\n"," ('sri', 4.515717029571533),\n"," ('australia', 4.806082725524902),\n"," ('thailand', 4.994781494140625),\n"," ('malaysia', 5.009334087371826)]"]},"metadata":{"tags":[]},"execution_count":107}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dy1br1mQGHq5","executionInfo":{"status":"ok","timestamp":1616057014838,"user_tz":-360,"elapsed":4627,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"31e04b8e-9b98-44d0-82ea-80b32b4f2ac1"},"source":["word_vector = get_vector(glove, 'google')\n","\n","closest_words(glove, word_vector)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('google', 0.0),\n"," ('yahoo', 3.0772178173065186),\n"," ('microsoft', 3.8836112022399902),\n"," ('web', 4.10483980178833),\n"," ('aol', 4.108161449432373),\n"," ('facebook', 4.116486072540283),\n"," ('ebay', 4.39174222946167),\n"," ('msn', 4.412169933319092),\n"," ('internet', 4.4540276527404785),\n"," ('netscape', 4.465073108673096)]"]},"metadata":{"tags":[]},"execution_count":110}]},{"cell_type":"markdown","metadata":{"id":"_aBwPpmUxRUk"},"source":["## Analogies\n","\n","Another property of word embeddings is that they can be operated on just as any standard vector and give interesting results."]},{"cell_type":"code","metadata":{"id":"ItPm3L48xWVj"},"source":["def analogy(embeddings, word1, word2, word3, n=4):\n","    \n","    #get vectors for each word\n","    word1_vector = get_vector(embeddings, word1)\n","    word2_vector = get_vector(embeddings, word2)\n","    word3_vector = get_vector(embeddings, word3)\n","    \n","    #calculate analogy vector\n","    analogy_vector = word2_vector - word1_vector + word3_vector\n","    \n","    #find closest words to analogy vector\n","    candidate_words = closest_words(embeddings, analogy_vector, n+3)\n","    \n","    #filter out words already in analogy\n","    candidate_words = [(word, dist) for (word, dist) in candidate_words \n","                       if word not in [word1, word2, word3]][:n]\n","    \n","    print(f'{word1} is to {word2} as {word3} is to...')\n","    \n","    return candidate_words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mx8I61GGx-4y"},"source":["<div align=\"center\">\n","<img src=\"https://drive.google.com/uc?id=12Kku3uSvqqaTya7trjkfy5EKU7pC9u2U\" width=\"500\">\n","</div>\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vwAE_EzhxjRF","executionInfo":{"status":"ok","timestamp":1616057278015,"user_tz":-360,"elapsed":5629,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"eea7a61c-a829-479a-b23d-9929185afd83"},"source":["print(analogy(glove, 'man', 'king', 'woman'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["man is to king as woman is to...\n","[('queen', 4.08107852935791), ('monarch', 4.642907619476318), ('throne', 4.905500888824463), ('elizabeth', 4.921558380126953)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"39ZCvfp6yUhw"},"source":["If we think about it, the vector calculated from 'king' minus 'man' gives us a \"royalty vector\". This is the vector associated with traveling from a man to his royal counterpart, a king. If we add this \"royality vector\" to 'woman', this should travel to her royal equivalent, which is a queen!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N4LMvlaGylbo","executionInfo":{"status":"ok","timestamp":1616057470689,"user_tz":-360,"elapsed":4637,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"a4f77cd4-e36f-4b8f-ce83-60ed763147a8"},"source":["print(analogy(glove, 'man', 'actor', 'woman'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["man is to actor as woman is to...\n","[('actress', 2.8133397102355957), ('comedian', 5.003941059112549), ('actresses', 5.139926433563232), ('starred', 5.277286052703857)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8F_bPvxNyvPi","executionInfo":{"status":"ok","timestamp":1616057486213,"user_tz":-360,"elapsed":4643,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"1c3f2357-3937-4d89-e011-e4157da6365a"},"source":["print(analogy(glove, 'india', 'delhi', 'bangladesh'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["india is to delhi as bangladesh is to...\n","[('dhaka', 3.076033353805542), ('kathmandu', 4.292879104614258), ('lahore', 4.358791351318359), ('bangladeshi', 4.5970611572265625)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RgQ5gUXEGWC7","executionInfo":{"status":"ok","timestamp":1616057498516,"user_tz":-360,"elapsed":4529,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"28bb43aa-a6f5-44bb-9b52-ac89d9a480da"},"source":["print(analogy(glove, 'good', 'heaven', 'bad'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["good is to heaven as bad is to...\n","[('hell', 4.395862102508545), ('ghosts', 5.286444187164307), ('hades', 5.289849281311035), ('madness', 5.341413497924805)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bvTQLmNcGgr1","executionInfo":{"status":"ok","timestamp":1616057512742,"user_tz":-360,"elapsed":4722,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"1537f36b-0d1f-4159-e0a0-b13b7f7ef8db"},"source":["print(analogy(glove, 'jordan', 'basketball', 'ronaldo'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["jordan is to basketball as ronaldo is to...\n","[('soccer', 6.455689907073975), ('romario', 6.579685688018799), ('beckham', 6.839649677276611), ('ronaldinho', 6.978160381317139)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mkQ1Ns5WHIpC","executionInfo":{"status":"ok","timestamp":1616057523033,"user_tz":-360,"elapsed":4945,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"a5915cd2-db55-4c50-85b1-a14230b8f372"},"source":["print(analogy(glove, 'paper', 'newspaper', 'screen'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["paper is to newspaper as screen is to...\n","[('tv', 4.780970096588135), ('television', 5.104853630065918), ('cinema', 5.381847858428955), ('feature', 5.552447319030762)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RcTR-Q-wz8Co"},"source":["## Similarity operations on embeddings"]},{"cell_type":"code","metadata":{"id":"HL0AdBkKz9Ol"},"source":["from scipy import spatial\n","\n","def cosineSim(word1, word2):\n","    vector1, vector2 = get_vector(glove, word1), get_vector(glove, word2)\n","    return 1 - spatial.distance.cosine(vector1, vector2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y-GiNcnb0t0S","executionInfo":{"status":"ok","timestamp":1616057627047,"user_tz":-360,"elapsed":976,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"7bc7159c-c8c7-4da7-b137-bd8eaa82f0fd"},"source":["word_pairs = [\n","    ('dog', 'cat'),\n","    ('tree', 'cat'),\n","    ('tree', 'leaf'),\n","    ('king', 'queen'),\n","]\n","\n","for word1, word2 in word_pairs:\n","    print(f'Similarity between \"{word1}\" and \"{word2}\":\\t{cosineSim(word1, word2):.2f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Similarity between \"dog\" and \"cat\":\t0.88\n","Similarity between \"tree\" and \"cat\":\t0.45\n","Similarity between \"tree\" and \"leaf\":\t0.64\n","Similarity between \"king\" and \"queen\":\t0.75\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"10cb1WhyrXl_"},"source":["### Need to learn embedding for your own corpus? \n","\n","#### Simplest Ans: Use [Gensim Library](https://radimrehurek.com/gensim/auto_examples/index.html#documentation)\n","\n","-  [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)\n","-  [fastText](https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html)\n","-  [Doc2Vec](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html)\n","-  [GloVe](https://nlp.stanford.edu/projects/glove/)\n","-  [How is GloVe different from word2vec?](https://www.quora.com/How-is-GloVe-different-from-word2vec)"]},{"cell_type":"markdown","metadata":{"id":"yF0-IdrzkLbQ"},"source":["\n","### Job Related Topics - Part I [Optional]\n","\n","- Create a professional email address \n","    - First name + last name = firstlast@domain.com\n","    - First name . last name = first.last@domain.com\n","    - First name - last name = first-last@domain.com\n","    - First name . middle name . last name = first.middle.last@domain.com\n","    - First name - middle name - last name = first-middle-last@domain.com\n","    - First initial + last name = flast@domain.com\n","    - First initial + middle name + last name = fmiddlelast@domain.com\n","    - First initial + middle initial + last name = fmlast@domain.com\n","- The shorter your email the better\n","- Complete your Linkedin profile\n","- Prepare a CV in Latex \n","- Seperate your contact number [personal vs professional]\n","- Create GitHub profile [Username may only contain alphanumeric characters or single hyphens, and cannot begin or end with a hyphen.]\n","- You can also use [desktop version of GitHub](https://desktop.github.com/). It's very easy to use without any commands!\n","- Build your website using [GitHub pages](https://pages.github.com/)\n","- [Great Templates! ](https://wowchemy.com/templates/) to use. "]}]}