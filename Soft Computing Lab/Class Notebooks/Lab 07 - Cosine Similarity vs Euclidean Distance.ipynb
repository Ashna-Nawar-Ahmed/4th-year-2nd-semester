{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab 07 - Cosine Similarity vs Euclidean Distance.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPrzJyH6SGE2DzMGKcJc9la"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"aNOCKHK_BODL"},"source":["## Cosine Similarity & Euclidean Distance\n","\n","Consider the following 4 sentences:- \n","\n","- It was the best of times.\n","- it was the worst of Times.\n","- it is the time of stupidity.\n","- it is the age of foolishness."]},{"cell_type":"code","metadata":{"id":"9zWXGpE-hkzs","executionInfo":{"status":"ok","timestamp":1615881115225,"user_tz":-360,"elapsed":875,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}}},"source":["# Define the documents\n","doc1 = \"It was the best of times.\"\n","\n","doc2 = \"it was the worst of Times.\"\n","\n","doc3  = \"it is the time of stupidity.\"\n","\n","doc4  = \"it is the age of foolishness.\"\n","\n","documents = [doc1, doc2, doc3, doc4]"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yzk6gUiYQVLR","colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"status":"ok","timestamp":1615881272894,"user_tz":-360,"elapsed":1888,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"2bbddbf4-d864-4acc-ba31-4dc0b87b09b6"},"source":["# Scikit Learn using Bag of Words\n","from sklearn.feature_extraction.text import CountVectorizer\n","import pandas as pd\n","\n","# Create the Document Term Matrix\n","bag_of_words_vectorizer = CountVectorizer()\n","bag_of_words = bag_of_words_vectorizer.fit_transform(documents).toarray()\n","\n","# Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n","df = pd.DataFrame(bag_of_words, \n","                  columns=bag_of_words_vectorizer.get_feature_names(),\n","                  index=['doc1', 'doc2', 'doc3', 'doc4'])\n","\n","print(bag_of_words)\n","display(df)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[[0 1 0 0 1 1 0 1 0 1 1 0]\n"," [0 0 0 0 1 1 0 1 0 1 1 1]\n"," [0 0 0 1 1 1 1 1 1 0 0 0]\n"," [1 0 1 1 1 1 0 1 0 0 0 0]]\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>best</th>\n","      <th>foolishness</th>\n","      <th>is</th>\n","      <th>it</th>\n","      <th>of</th>\n","      <th>stupidity</th>\n","      <th>the</th>\n","      <th>time</th>\n","      <th>times</th>\n","      <th>was</th>\n","      <th>worst</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>doc1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>doc2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>doc3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>doc4</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      age  best  foolishness  is  it  ...  the  time  times  was  worst\n","doc1    0     1            0   0   1  ...    1     0      1    1      0\n","doc2    0     0            0   0   1  ...    1     0      1    1      1\n","doc3    0     0            0   1   1  ...    1     1      0    0      0\n","doc4    1     0            1   1   1  ...    1     0      0    0      0\n","\n","[4 rows x 12 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"H95M6LXagaaI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615881634452,"user_tz":-360,"elapsed":930,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"28366db4-d1b3-4b0a-f855-f0ce66510d90"},"source":["from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n","\n","# Compute Cosine Similarity\n","print(\"Cosine Similarity\")\n","print(cosine_similarity(df, df))\n","\n","# Compute Euclidean Distance\n","print(\"Euclidean Distance\")\n","print(euclidean_distances(df, df))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Cosine Similarity\n","[[1.         0.83333333 0.5        0.5       ]\n"," [0.83333333 1.         0.5        0.5       ]\n"," [0.5        0.5        1.         0.66666667]\n"," [0.5        0.5        0.66666667 1.        ]]\n","Euclidean Distance\n","[[0.         1.41421356 2.44948974 2.44948974]\n"," [1.41421356 0.         2.44948974 2.44948974]\n"," [2.44948974 2.44948974 0.         2.        ]\n"," [2.44948974 2.44948974 2.         0.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L_Xr1h7qgGkh"},"source":["## Improvement using Stopword Filtering and Lemmatization"]},{"cell_type":"code","metadata":{"id":"8zg9oO8hQmqF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615881812656,"user_tz":-360,"elapsed":1353,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"612f9ba5-8827-4d1f-d166-f33f4f6271f7"},"source":["import spacy\n","\n","# Small spaCy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","lemma_documents = []\n","\n","# Lemmatize docs (individual)\n","def lemmatize_docs(x):\n","  x = x.lower()\n","  doc = nlp(x)\n","  lemma_words = [w.lemma_ if w.lemma_ !='-PRON-' else w.text for w in doc]\n","  return \" \".join(lemma_words)\n","\n","for doc in documents:\n","  lemma_documents.append(lemmatize_docs(doc))\n","\n","print(lemma_documents)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["['it be the good of time .', 'it be the bad of time .', 'it be the time of stupidity .', 'it be the age of foolishness .']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i1gl9lBnUJ9l","colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"status":"ok","timestamp":1615881888269,"user_tz":-360,"elapsed":787,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"32932655-aade-4569-bb50-834dc1b3b680"},"source":["# Scikit Learn using Bag of Words\n","from sklearn.feature_extraction.text import CountVectorizer\n","import pandas as pd\n","\n","# Create the Document Term Matrix\n","bag_of_words_vectorizer = CountVectorizer(stop_words='english')\n","bag_of_words = bag_of_words_vectorizer.fit_transform(lemma_documents).toarray()\n","\n","# Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n","df = pd.DataFrame(bag_of_words, \n","                  columns=bag_of_words_vectorizer.get_feature_names(),\n","                  index=['doc1', 'doc2', 'doc3', 'doc4'])\n","\n","print(bag_of_words)\n","display(df)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[[0 0 0 1 0 1]\n"," [0 1 0 0 0 1]\n"," [0 0 0 0 1 1]\n"," [1 0 1 0 0 0]]\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>bad</th>\n","      <th>foolishness</th>\n","      <th>good</th>\n","      <th>stupidity</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>doc1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>doc2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>doc3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>doc4</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      age  bad  foolishness  good  stupidity  time\n","doc1    0    0            0     1          0     1\n","doc2    0    1            0     0          0     1\n","doc3    0    0            0     0          1     1\n","doc4    1    0            1     0          0     0"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"CzaSNcvJgATZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615881912376,"user_tz":-360,"elapsed":913,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}},"outputId":"d8369f37-5016-46aa-91c3-f11cc5e4b721"},"source":["from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n","\n","# Compute Cosine Similarity\n","print(\"Cosine Similarity\")\n","print(cosine_similarity(df, df))\n","\n","# Compute Euclidean Distance\n","print(\"Euclidean Distance\")\n","print(euclidean_distances(df, df))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Cosine Similarity\n","[[1.  0.5 0.5 0. ]\n"," [0.5 1.  0.5 0. ]\n"," [0.5 0.5 1.  0. ]\n"," [0.  0.  0.  1. ]]\n","Euclidean Distance\n","[[0.         1.41421356 1.41421356 2.        ]\n"," [1.41421356 0.         1.41421356 2.        ]\n"," [1.41421356 1.41421356 0.         2.        ]\n"," [2.         2.         2.         0.        ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pDUB5xce0tmP","executionInfo":{"status":"ok","timestamp":1615851871969,"user_tz":-360,"elapsed":4359,"user":{"displayName":"Mir Tafseer Nayeem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiM4cOpMNCedgjDWZ9oHIKufBTjojcruiTtinn=s64","userId":"11387451028968277735"}}},"source":[""],"execution_count":12,"outputs":[]}]}